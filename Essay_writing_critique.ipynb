{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a53c8272-8465-4398-be51-31ca3747c7ca",
   "metadata": {},
   "source": [
    "## Lesson 6: Essay Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc7b707d-ae00-4ce9-9857-2da65bd559c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6856827f-623b-49f2-9ef3-a1c9c26377bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53563d3a-b4e7-4101-bfb4-ea3e16407eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Tavily API key:\n",
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "if not os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Tavily API key:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29223679-21bf-4e76-930c-0522ed121e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "OPENAI API key:\n",
      " ········\n"
     ]
    }
   ],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI API key:\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d82b3ab2-d72f-41c9-8a49-ebc145aa7ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77b79d5a-7fc9-4ded-b625-08973bec4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d690dd70-e5be-4a7f-9301-57e060ce8dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay. \\\n",
    "Write such an outline for the user provided topic. Give an outline of the essay along with any relevant notes \\\n",
    "or instructions for the sections.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9591e359-f58a-4bc0-a78b-c4009e23b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 5-paragraph essays.\\\n",
    "Generate the best essay possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of your previous attempts. \\\n",
    "Utilize all the information below as needed: \n",
    "\n",
    "------\n",
    "\n",
    "{content}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bbe169a-8a32-4913-9e04-d646b59c2fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are an essay assistant tasked with writing excellent 5-paragraph essays.Generate the best essay possible for the user's request and the initial outline. If the user provides critique, respond with a revised version of your previous attempts. Utilize all the information below as needed: \\n\\n------\\n\\n{content}\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WRITER_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc056bfe-2b39-44bf-87ac-c231272bb231",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0902f615-4636-4159-95b0-05f9a8af27b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when writing the following essay. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f97dc5b7-06f6-40c1-a276-c094c10c67ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as outlined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. Only generate 3 queries max.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1712271a-1001-4a8f-9aaf-dba15d394b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a6d4f88-1dce-47af-9163-97517b3c69d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"TAVILY_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c205f5da-86fc-4518-a7ac-7c1f661eec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "import os\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f13b3207-6358-4726-952e-7b9ebee2cdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT), \n",
    "        HumanMessage(content=state['task'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"plan\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "449cb082-7fc5-4d6b-8697-663aaa5a3a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_plan_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
    "        HumanMessage(content=state['task'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9357c1d-42f2-439b-a4bd-3523e6c54aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join(state['content'] or [])\n",
    "    user_message = HumanMessage(\n",
    "        content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=WRITER_PROMPT.format(content=content)\n",
    "        ),\n",
    "        user_message\n",
    "        ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\n",
    "        \"draft\": response.content, \n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e008e762-0556-4240-bae6-a7c179de8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT), \n",
    "        HumanMessage(content=state['draft'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"critique\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1882856-669f-49cd-9be7-b2496982edf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_critique_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
    "        HumanMessage(content=state['critique'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09b39a2d-8db7-4264-809a-d39301413f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4969074-76e6-4ce9-aeb6-c8b92431e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "387f7f85-3f86-4f41-a696-a9484fd882d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5fd64fb-9b3b-4c06-aac2-962d97144155",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.set_entry_point(\"planner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa4fac26-5fb0-4df0-8103-705d26443422",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.add_conditional_edges(\n",
    "    \"generate\", \n",
    "    should_continue, \n",
    "    {END: END, \"reflect\": \"reflect\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "018e0ce3-ee34-4f30-afc8-f9582e0d6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2dc914c-09bd-4dc4-aa9f-291aa8c8e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326367b-b9e5-4b96-bd41-7a00c74de593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "\n",
    "#Image(graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40ab48ce-4745-4cdb-a7c3-7f8726702038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'planner': {'plan': 'I. Introduction\\n    A. Brief overview of Langchain and Langsmith\\n    B. Thesis statement: Exploring the differences between Langchain and Langsmith\\n\\nII. Langchain\\n    A. Definition and explanation\\n    B. Key features and characteristics\\n    C. Use cases and applications\\n    D. Advantages and disadvantages\\n\\nIII. Langsmith\\n    A. Definition and explanation\\n    B. Key features and characteristics\\n    C. Use cases and applications\\n    D. Advantages and disadvantages\\n\\nIV. Comparison between Langchain and Langsmith\\n    A. Technology stack\\n    B. Scalability\\n    C. Security\\n    D. Interoperability\\n    E. Performance\\n\\nV. Conclusion\\n    A. Recap of main differences between Langchain and Langsmith\\n    B. Implications for future developments in the field\\n    C. Final thoughts on the significance of understanding these differences'}}\n",
      "{'research_plan': {'content': ['Langchain vs Langsmith: Unpacking the AI Language Model Showdown – Be on the Right Side of Change Langchain vs Langsmith: Unpacking the AI Language Model Showdown If Langchain is the engine, LangSmith is the dashboard helping you monitor and debug the performance of your LLM applications. It’s about bringing your ideas from initial prototype to a robust production environment, using tools like LangSmith for crafting and LangChain for deployment. Remember, while LangSmith helps you build, test, and debug your prototype, LangChain offers the structure for deploying and monitoring your final product in a live environment. Whether you’re trying to figure out which tool fits your needs or you’re just getting started with language model automation, these FAQs will help shed light on the common curiosities about Langchain and LangSmith.', 'But when it’s time to troubleshoot or improve your workflows, LangSmith steps in with tools to analyze, debug, and optimize. When it comes to practical application, I always say: “Show, don’t tell.” I’ve used both LangChain and LangSmith extensively, and I’ve found that they complement each other beautifully when you’re building and fine-tuning LLM-based workflows. Having spent countless hours building and debugging LLM-based systems, I’ve learned that using LangChain and LangSmith effectively requires a few smart strategies. I often use LangChain to build my pipelines and LangSmith to monitor and debug them. Start with LangChain to build your pipeline, and then bring in LangSmith to ensure it performs as expected.', 'LangChain vs LangSmith: Understanding the Differences, Pros, and Cons | by Ajay Verma | GoPenAI LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. In this blog, we’ll delve into the differences between LangChain and LangSmith, their pros and cons, and when to use each one. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities.', 'But when it’s time to troubleshoot or improve your workflows, LangSmith steps in with tools to analyze, debug, and optimize. When it comes to practical application, I always say: “Show, don’t tell.” I’ve used both LangChain and LangSmith extensively, and I’ve found that they complement each other beautifully when you’re building and fine-tuning LLM-based workflows. Having spent countless hours building and debugging LLM-based systems, I’ve learned that using LangChain and LangSmith effectively requires a few smart strategies. I often use LangChain to build my pipelines and LangSmith to monitor and debug them. Start with LangChain to build your pipeline, and then bring in LangSmith to ensure it performs as expected.', 'LangChain is an open-source framework designed to streamline the development of applications leveraging language models. It offers modular components to connect various tasks, enabling a seamless', 'But when it’s time to troubleshoot or improve your workflows, LangSmith steps in with tools to analyze, debug, and optimize. When it comes to practical application, I always say: “Show, don’t tell.” I’ve used both LangChain and LangSmith extensively, and I’ve found that they complement each other beautifully when you’re building and fine-tuning LLM-based workflows. Having spent countless hours building and debugging LLM-based systems, I’ve learned that using LangChain and LangSmith effectively requires a few smart strategies. I often use LangChain to build my pipelines and LangSmith to monitor and debug them. Start with LangChain to build your pipeline, and then bring in LangSmith to ensure it performs as expected.']}}\n",
      "{'generate': {'draft': \"**Title: Langchain vs Langsmith: Unpacking the AI Language Model Showdown**\\n\\nI. **Introduction**\\nIn the realm of AI language models, Langchain and Langsmith play pivotal roles. Langchain serves as the engine, while Langsmith acts as the dashboard for monitoring and debugging LLM applications. This essay aims to delve into the distinctions between Langchain and Langsmith to provide a comprehensive understanding of their functionalities.\\n\\nII. **Langchain**\\nLangchain is an open-source framework tailored to streamline the development of applications utilizing language models. It offers modular components that facilitate the connection of various tasks, ensuring a seamless workflow. Key features include its versatility in connecting tasks and its ability to enhance the efficiency of application development. Langchain is ideal for early-stage prototyping and small-scale applications due to its agility and ease of use.\\n\\nIII. **Langsmith**\\nOn the other hand, Langsmith serves as a comprehensive platform designed to manage all aspects of LLM development. It is particularly well-suited for large-scale, production-ready applications that demand advanced debugging, testing, and monitoring capabilities. Langsmith steps in when troubleshooting or optimizing workflows is necessary, providing tools for analysis and debugging to ensure optimal performance.\\n\\nIV. **Comparison between Langchain and Langsmith**\\n- **Technology Stack:** Langchain focuses on streamlining development with modular components, while Langsmith offers a unified platform for managing all LLM development aspects.\\n- **Scalability:** Langchain is more agile and suitable for small-scale applications, whereas Langsmith excels in handling large-scale, production-ready projects.\\n- **Security:** Both Langchain and Langsmith prioritize security; however, Langsmith's advanced capabilities cater better to stringent security requirements.\\n- **Interoperability:** Langchain and Langsmith can be integrated with various systems, but Langsmith's comprehensive platform enhances interoperability.\\n- **Performance:** While Langchain is efficient for prototyping, Langsmith's advanced tools ensure optimal performance in production environments.\\n\\nV. **Conclusion**\\nIn conclusion, understanding the nuances between Langchain and Langsmith is crucial for selecting the right tool for your AI language model projects. While Langchain is ideal for agile development and prototyping, Langsmith shines in managing large-scale applications with advanced debugging and monitoring features. By leveraging the strengths of both tools, developers can navigate the complexities of LLM development effectively, ensuring successful outcomes in the ever-evolving landscape of AI technology.\", 'revision_number': 2}}\n",
      "{'reflect': {'critique': '**Critique:**\\n\\n1. **Introduction:**\\nThe introduction effectively sets the stage for the comparison between Langchain and Langsmith. However, it could be enhanced by providing a brief overview of the significance of AI language models in the current technological landscape and why understanding the differences between Langchain and Langsmith is important.\\n\\n2. **Content:**\\n- **Langchain and Langsmith Descriptions:** The descriptions of Langchain and Langsmith are clear and informative, highlighting their key features and target applications effectively.\\n- **Comparison Section:** The comparison section provides a structured breakdown of the differences between Langchain and Langsmith across various aspects, which is essential for readers to grasp the distinctions between the two tools.\\n\\n3. **Depth and Detail:**\\nWhile the essay provides a good overview of Langchain and Langsmith, it could benefit from more in-depth analysis and examples to illustrate how each tool functions in real-world scenarios. Providing case studies or specific use cases would enhance the depth of the comparison.\\n\\n4. **Recommendations:**\\n- **Include Real-World Examples:** Incorporating real-world examples or case studies showcasing the application of Langchain and Langsmith would make the essay more engaging and practical for readers.\\n- **Expand on Scalability and Security:** Delve deeper into how Langchain and Langsmith handle scalability and security, providing specific features or mechanisms that demonstrate their capabilities in these areas.\\n- **Discuss User Experience:** Consider discussing the user experience aspects of Langchain and Langsmith, such as ease of use, learning curve, and community support, to provide a holistic view for potential users.\\n- **Conclusion Enhancement:** Strengthen the conclusion by summarizing the key points of differentiation between Langchain and Langsmith and reiterating the importance of selecting the right tool based on project requirements.\\n\\n5. **Length and Structure:**\\nWhile the essay is concise and structured well, expanding on each section with more detailed explanations and examples would enhance the overall depth and quality of the comparison.\\n\\nOverall, the essay provides a solid foundation for understanding the differences between Langchain and Langsmith. By incorporating more depth, real-world examples, and expanding on key aspects, the essay can offer a more comprehensive analysis for readers seeking insights into these AI language model tools.'}}\n",
      "{'research_critique': {'content': ['Langchain vs Langsmith: Unpacking the AI Language Model Showdown – Be on the Right Side of Change Langchain vs Langsmith: Unpacking the AI Language Model Showdown If Langchain is the engine, LangSmith is the dashboard helping you monitor and debug the performance of your LLM applications. It’s about bringing your ideas from initial prototype to a robust production environment, using tools like LangSmith for crafting and LangChain for deployment. Remember, while LangSmith helps you build, test, and debug your prototype, LangChain offers the structure for deploying and monitoring your final product in a live environment. Whether you’re trying to figure out which tool fits your needs or you’re just getting started with language model automation, these FAQs will help shed light on the common curiosities about Langchain and LangSmith.', 'But when it’s time to troubleshoot or improve your workflows, LangSmith steps in with tools to analyze, debug, and optimize. When it comes to practical application, I always say: “Show, don’t tell.” I’ve used both LangChain and LangSmith extensively, and I’ve found that they complement each other beautifully when you’re building and fine-tuning LLM-based workflows. Having spent countless hours building and debugging LLM-based systems, I’ve learned that using LangChain and LangSmith effectively requires a few smart strategies. I often use LangChain to build my pipelines and LangSmith to monitor and debug them. Start with LangChain to build your pipeline, and then bring in LangSmith to ensure it performs as expected.', 'LangChain vs LangSmith: Understanding the Differences, Pros, and Cons | by Ajay Verma | GoPenAI LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. In this blog, we’ll delve into the differences between LangChain and LangSmith, their pros and cons, and when to use each one. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities.', 'But when it’s time to troubleshoot or improve your workflows, LangSmith steps in with tools to analyze, debug, and optimize. When it comes to practical application, I always say: “Show, don’t tell.” I’ve used both LangChain and LangSmith extensively, and I’ve found that they complement each other beautifully when you’re building and fine-tuning LLM-based workflows. Having spent countless hours building and debugging LLM-based systems, I’ve learned that using LangChain and LangSmith effectively requires a few smart strategies. I often use LangChain to build my pipelines and LangSmith to monitor and debug them. Start with LangChain to build your pipeline, and then bring in LangSmith to ensure it performs as expected.', 'LangChain is an open-source framework designed to streamline the development of applications leveraging language models. It offers modular components to connect various tasks, enabling a seamless', 'But when it’s time to troubleshoot or improve your workflows, LangSmith steps in with tools to analyze, debug, and optimize. When it comes to practical application, I always say: “Show, don’t tell.” I’ve used both LangChain and LangSmith extensively, and I’ve found that they complement each other beautifully when you’re building and fine-tuning LLM-based workflows. Having spent countless hours building and debugging LLM-based systems, I’ve learned that using LangChain and LangSmith effectively requires a few smart strategies. I often use LangChain to build my pipelines and LangSmith to monitor and debug them. Start with LangChain to build your pipeline, and then bring in LangSmith to ensure it performs as expected.', 'Emerging language models are shaping the future of artificial intelligence. These advancements bring significant improvements in understanding human language, affecting society, and pushing the technological envelope.', 'The application of language models is diverse and includes text completion, text-to-speech conversion, language translation, chatbots, virtual assistants, and speech recognition. This report offers an overview of the AI language model and NLP landscape with current and emerging policy responses from around the world.', 'But when it’s time to troubleshoot or improve your workflows, LangSmith steps in with tools to analyze, debug, and optimize. When it comes to practical application, I always say: “Show, don’t tell.” I’ve used both LangChain and LangSmith extensively, and I’ve found that they complement each other beautifully when you’re building and fine-tuning LLM-based workflows. Having spent countless hours building and debugging LLM-based systems, I’ve learned that using LangChain and LangSmith effectively requires a few smart strategies. I often use LangChain to build my pipelines and LangSmith to monitor and debug them. Start with LangChain to build your pipeline, and then bring in LangSmith to ensure it performs as expected.', 'from langchain.tools import DuckDuckGoSearchRun from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.memory import ConversationBufferMemory from langchain.schema import SystemMessage import os os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\" # Replace with your API Key search_tool = DuckDuckGoSearchRun() llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) memory = ConversationBufferMemory(return_messages=True) response_prompt = ChatPromptTemplate.from_messages([ SystemMessage(content=\"You are a helpful and knowledgeable AI assistant.\"), (\"user\", \"{input}\\\\n\\\\nSearch Results: {search_results}\\\\n\\\\nChat History:\\\\n{chat_history}\") ]) def search(state: AgentState): query = state[\"input\"] results = search_tool.run(query) return {\"search_results\": results} def generate_response(state: AgentState): inputs = state inputs[\"chat_history\"] = memory.load_memory_variables({})[\"history\"] prompt = response_prompt.format_messages(**inputs) response = llm(prompt).content memory.save_context({\"input\": state[\"input\"]},{\"output\":response}) return {\"output\":response} From building foundational LLM applications with LangChain to modeling intricate stateful agents with LangGraph and ensuring reliability with LangSmith, these tools are indispensable in the modern landscape of AI development.', 'If you’re responsible for ensuring your AI models work in production, or you need to frequently debug and monitor your pipelines, Langsmith is your go-to tool. In short, while Langchain excels at managing and scaling model workflows, Langsmith is designed for those times when you need deep visibility and control over large, complex AI systems in production. But if you’re managing a complex AI pipeline with multiple models that need debugging and orchestrating, Langsmith’s capabilities become essential. If you’re debugging complex AI models or managing large-scale workflows with multiple moving parts, Langsmith’s advanced debugging and orchestration features will be indispensable. Choosing between Langchain and Langsmith boils down to understanding your project’s complexity, scalability needs, and the level of control you want over your AI models.', 'Cloud architecture and scalability | 🦜️🛠️ LangSmith This section is only relevant for the cloud-managed LangSmith services available at https://smith.langchain.com and https://eu.smith.langchain.com. LangSmith is deployed on Google Cloud Platform (GCP) and is designed to be highly scalable. The US-based LangSmith service is deployed in the us-central1 (Iowa) region of GCP. NOTE: The EU-based LangSmith service is now available (as of mid-July 2024) and is deployed in the europe-west4 (Netherlands) region of GCP. Cloud-managed LangSmith uses Supabase for authentication/authorization and ClickHouse Cloud for data warehouse. LangSmith uses the following GCP storage services: Our services connect to Clickhouse Cloud, which is hosted in the same GCP region, via a private endpoint. Google Cloud Load Balancer for routing traffic to the LangSmith services.']}}\n",
      "{'generate': {'draft': \"**Title: Langchain vs Langsmith: Unpacking the AI Language Model Showdown**\\n\\nI. **Introduction**\\n   \\n   In the realm of AI language models, Langchain and Langsmith stand out as essential tools for developers. Langchain serves as the engine, while Langsmith acts as the dashboard for monitoring and debugging LLM applications. This essay aims to delve into the distinctions between Langchain and Langsmith to provide a comprehensive understanding of their roles in AI development.\\n\\nII. **Langchain**\\n\\n    A. **Definition and Explanation**\\n    \\n    Langchain is an open-source framework designed to streamline the development of applications utilizing language models. It offers modular components that facilitate the connection of various tasks, ensuring a seamless workflow.\\n    \\n    B. **Key Features and Characteristics**\\n    \\n    Langchain provides the structure for deploying and monitoring final LLM products in a live environment. It is ideal for early-stage prototyping and small-scale applications.\\n    \\n    C. **Use Cases and Applications**\\n    \\n    Langchain is commonly used for building foundational LLM applications and simplifying the development process.\\n    \\n    D. **Advantages and Disadvantages**\\n    \\n    Advantages of Langchain include its open-source nature and ease of use for prototyping. However, it may lack the advanced debugging and monitoring capabilities required for large-scale applications.\\n\\nIII. **Langsmith**\\n\\n    A. **Definition and Explanation**\\n    \\n    Langsmith is a tool that aids in analyzing, debugging, and optimizing LLM workflows. It provides advanced tools for troubleshooting and improving the performance of language model applications.\\n    \\n    B. **Key Features and Characteristics**\\n    \\n    Langsmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications.\\n    \\n    C. **Use Cases and Applications**\\n    \\n    Langsmith is best suited for scenarios where deep visibility and control over complex AI systems in production are necessary.\\n    \\n    D. **Advantages and Disadvantages**\\n    \\n    The advantages of Langsmith lie in its advanced debugging and orchestration features, crucial for managing complex AI pipelines. However, it may not be as suitable for early-stage prototyping.\\n\\nIV. **Comparison between Langchain and Langsmith**\\n\\n    A. **Technology Stack**\\n    \\n    Langchain focuses on streamlining development, while Langsmith emphasizes debugging and optimization.\\n    \\n    B. **Scalability**\\n    \\n    Langchain is ideal for small-scale applications, whereas Langsmith excels in large-scale, production-ready scenarios.\\n    \\n    C. **Security**\\n    \\n    Both tools prioritize security, but Langsmith's advanced features may offer more robust security measures.\\n    \\n    D. **Interoperability**\\n    \\n    Langchain and Langsmith can work together seamlessly to cover the entire development lifecycle of LLM applications.\\n    \\n    E. **Performance**\\n    \\n    Langchain may offer quicker prototyping capabilities, while Langsmith ensures optimized performance in production environments.\\n\\nV. **Conclusion**\\n\\n    A. **Recap of Main Differences**\\n    \\n    Langchain and Langsmith play distinct roles in the development of AI language models, with Langchain focusing on prototyping and Langsmith on production-ready applications.\\n    \\n    B. **Implications for Future Developments**\\n    \\n    Understanding the differences between Langchain and Langsmith is crucial for developers to choose the right tool for their specific needs and project requirements.\\n    \\n    C. **Final Thoughts**\\n    \\n    The significance of Langchain and Langsmith in the AI development landscape cannot be overstated, as they offer essential functionalities for building and deploying advanced language model applications.\", 'revision_number': 3}}\n"
     ]
    }
   ],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for s in graph.stream({\n",
    "    'task': \"what is the difference between langchain and langsmith\",\n",
    "    \"max_revisions\": 2,\n",
    "    \"revision_number\": 1,\n",
    "}, thread):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd7547-103e-4646-a9f4-2083220205d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
